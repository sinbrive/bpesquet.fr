<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Convolutional Neural Networks</title>

	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/css/reveal.css">
	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/css/theme/white.css">

	
	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/lib/css/zenburn.css">

	
	<link rel="stylesheet" href="https://www.bpesquet.fr/css/reveal.css">

	
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'https:\/\/www.bpesquet.fr\/reveal.js\/css\/print\/pdf.css' : 'https:\/\/www.bpesquet.fr\/reveal.js\/css\/print\/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
    <h1>Convolutional Neural Networks</h1>

    <p><a href="https://ensc.bordeaux-inp.fr">Ecole Nationale Sup√©rieure de Cognitique</a></p>

    <p><a href="https://www.bpesquet.fr">Baptiste Pesquet</a></p>

    
    
    <a href="https://github.com/bpesquet/machine-learning-katas"><img src="https://www.bpesquet.fr/images/GitHub-Mark-64px.png" alt="GitHub logo"></a>
    
    
</section>

			<section>

<h2 id="summary">Summary</h2>

<ul>
<li>CNN Architecture</li>
<li>CNN History</li>
</ul>

</section>
				<section>

<h2 id="cnn-architecture">CNN Architecture</h2>

</section>
				<section>

<h2 id="justification">Justification</h2>

<p>The visual world has the following properties:</p>

<ul>
<li>Translation invariance.</li>
<li>Spacial hierarchy: complex and abstract concepts are composed from simple elements.</li>
</ul>

<p>Classical models are not designed to detect local patterns in images.</p>

</section>
				<section>

<h2 id="general-design">General Design</h2>

<p><a href="https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"><img src="images/cnn_architecture.png" alt="General CNN architecture" /></a></p>

</section>
				<section>

<h2 id="the-convolution-operation">The convolution operation</h2>

<p>Apply a filter or <strong>kernel</strong> to data. Result is called a <strong>feature map</strong>.</p>

<p><a href="https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"><img src="images/convolution_overview.gif" alt="Convolution with a 3x3 filter of depth 1 applied on 5x5 data" /></a></p>

</section>
				<section>

<p><img src="images/convolution_example.jpeg" alt="Convolution example" /></p>

</section>
				<section>

<h2 id="convolution-parameters">Convolution parameters</h2>

<ul>
<li><strong>Filter dimensions</strong>: 2D for images.</li>
<li><strong>Filter size</strong>: generally 3x3 or 5x5.</li>
<li><strong>Number of filters</strong>: determine the number of feature maps created by the convolution operation.</li>
<li><strong>Stride</strong>: step for sliding the convolution window. Generally equal to 1.</li>
<li><strong>Padding</strong>: blank rows/columns with all-zero values added on sides of the input feature map.</li>
</ul>

</section>
				<section>

<h2 id="preserving-output-dimensions-with-padding">Preserving output dimensions with padding</h2>

<p><a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d"><img src="images/2d_convol.gif" alt="Preserving output dimensions with padding" /></a></p>

</section>
				<section>

<h2 id="2d-convolutions-on-3d-tensors">2D convolutions on 3D tensors</h2>

<ul>
<li>An image has several color channels.</li>
<li>Number of channels = filter depth.</li>
<li>The convolution result is still a scalar.</li>
</ul>

<p><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2"><img src="images/conv_image.png" alt="2D convolution on a 32x32x3 image with 10 filters" /></a></p>

</section>
				<section>

<p><a href="https://stackoverflow.com/a/44628011/2380880"><img src="images/2D_conv_over_rgb_image.png" alt="2D convolution over RGB image" /></a></p>

</section>
				<section>

<h2 id="activation-function">Activation function</h2>

<ul>
<li>Applied to the (scalar) convolution result.</li>
<li>Introduces non-linearity in the model.</li>
</ul>

<p><img src="images/relu.png" alt="ReLU" /></p>

</section>
				<section>

<h2 id="the-pooling-operation">The pooling operation</h2>

<ul>
<li>Reduces the dimensionality of feature maps.</li>
<li>Often done by selecting maximum values (<em>max pooling</em>).</li>
</ul>

<p><a href="https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks"><img src="images/maxpool_animation.gif" alt="Max pooling with 2x2 filter and stride of 2" /></a></p>

</section>
				<section>

<h2 id="pooling-output">Pooling output</h2>

<p><a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2"><img src="images/maxpooling_image.png" alt="Pooling with a 2x2 filter and stride of 2 on 10 32x32 feature maps" /></a></p>

</section>
				<section>

<h2 id="training-a-cnn">Training a CNN</h2>

<p>Same principle as a dense neural network: <a href="https://www.bpesquet.fr/en/slides/ai/neural-networks/">backpropagation</a> + <a href="https://www.bpesquet.fr/en/slides/ai/ml-fundamentals/">gradient descent</a>.</p>

<p><a href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/">Backpropagation In Convolutional Neural Networks</a></p>

</section>
				<section>

<h2 id="interpretation">Interpretation</h2>

<ul>
<li>Convolution layers act as feature extractors.</li>
<li>Dense layers use the extracted features to classify data.</li>
</ul>

</section>
				<section>

<p><a href="https://harishnarayanan.org/writing/artistic-style-transfer/"><img src="images/representation_learning.png" alt="Feature extraction with a CNN" /></a></p>

</section>
				<section>

<p><a href="https://transcranial.github.io/keras-js/#/mnist-cnn">Visualizing convnet layers on MNIST</a></p>

<p><a href="http://scs.ryerson.ca/~aharley/vis/conv/flat.html"><img src="images/conv_all_mnist.png" alt="Another visualization of intermediate layers on MNIST" /></a></p>

</section>
				<section>

<h2 id="cnn-history">CNN History</h2>

</section>
				<section>

<h2 id="the-beginnings-lenet5-1993">The beginnings: LeNet5 (1993)</h2>

<p><img src="images/lenet5.jpg" alt="LeNet5" /></p>

<div style="position: relative; height: 270px; width: 480px; overflow: hidden; margin: 0 auto 10px;">
    <iframe src="//www.youtube.com/embed/FwFduRA_L6Q" style="top: 0; left: 0; width: 100%; height: 100%;" allowfullscreen frameborder="0" title="YouTube Video"></iframe>
</div>
      

</section>
				<section>

<h2 id="the-breakthrough-ilsvrc">The breakthrough: ILSVRC</h2>

<ul>
<li><a href="http://image-net.org/challenges/LSVRC/"><em>ImageNet Large Scale Visual Recognition Challenge</em></a></li>
<li>Worldwide image classification challenge based on the <a href="http://www.image-net.org/">ImageNet</a> dataset.</li>
</ul>

<p><img src="images/ILSVRC_results.jpg" alt="ILSVRC results" /></p>

</section>
				<section>

<h2 id="alexnet-2012">AlexNet (2012)</h2>

<p>Trained on 2 GPU for 5 to 6 days.</p>

<p><img src="images/alexnet2.png" alt="AlexNet" /></p>

</section>
				<section>

<p><img src="images/go_deeper.png" alt="We need to go deeper" /></p>

</section>
				<section>

<h2 id="vgg-2014">VGG (2014)</h2>

<p><img src="images/vgg16.png" alt="VGG16" /></p>

</section>
				<section>

<h2 id="googlenet-inception-2014">GoogLeNet/Inception (2014)</h2>

<ul>
<li>9 Inception modules, more than 100 layers.</li>
<li>Trained on several GPU for about a week.</li>
</ul>

<p><img src="images/google_inception.jpg" alt="Inception" /></p>

</section>
				<section>

<h2 id="microsoft-resnet-2015">Microsoft ResNet (2015)</h2>

<ul>
<li>152 layers</li>
<li>Trained on 8 GPU for 2 to 3 weeks.</li>
<li>Smaller error rate than a average human.</li>
</ul>

<p><img src="images/resnet_archi.png" alt="ResNet" /></p>

</section>
				<section>

<p><img src="images/deeper_model.jpg" alt="Deeper model" /></p>
</section>
				</div>
	</div>
	<div>
		<a href="https://ensc.bordeaux-inp.fr">
			<img src="https://www.bpesquet.fr/images/ENSC.jpg" style="position: absolute;
						bottom: 10px;
						left: 10px;" />
		</a>
	</div>

	<script src="https://www.bpesquet.fr/reveal.js/lib/js/head.min.js"></script>
	<script src="https://www.bpesquet.fr/reveal.js/js/reveal.js"></script>

	<script>
		
		
		
		Reveal.initialize({
			
			history: false,
			
			slideNumber: true,
			
			transition: "convex",
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				config: 'TeX-MML-AM_HTMLorMML'  
			},
			dependencies: [
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/markdown\/marked.js' },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/markdown\/markdown.js' },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/notes\/notes.js', async: true },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/zoom-js\/zoom.js', async: true },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/highlight\/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/math\/math.js', async: true }
			]
		});
	</script>
</body>

</html>