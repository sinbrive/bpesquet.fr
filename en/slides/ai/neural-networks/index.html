<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>ML Algorithms: Neural Networks</title>

	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/css/reveal.css">
	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/css/theme/white.css">

	
	<link rel="stylesheet" href="https://www.bpesquet.fr/reveal.js/lib/css/zenburn.css">

	
	<link rel="stylesheet" href="https://www.bpesquet.fr/css/reveal.css">

	
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'https:\/\/www.bpesquet.fr\/reveal.js\/css\/print\/pdf.css' : 'https:\/\/www.bpesquet.fr\/reveal.js\/css\/print\/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
    <h1>ML Algorithms: Neural Networks</h1>

    <p><a href="https://ensc.bordeaux-inp.fr">Ecole Nationale Sup√©rieure de Cognitique</a></p>

    <p><a href="https://www.bpesquet.fr">Baptiste Pesquet</a></p>

    
    
    <a href="https://github.com/bpesquet/machine-learning-katas"><img src="https://www.bpesquet.fr/images/GitHub-Mark-64px.png" alt="GitHub logo"></a>
    
    
</section>

			<section>

<h2 id="summary">Summary</h2>

<ul>
<li>Introducing Neural Networks</li>
<li>Neural Network Tuning</li>
</ul>

</section>
				<section>

<h2 id="introducing-neural-networks">Introducing Neural Networks</h2>

</section>
				<section>

<section data-shortcode-section>
<h2 id="the-origins">The origins</h2>

<ul>
<li>1943 : first mathematical model of a biological neuron (McCulloch &amp; Pitts)</li>
<li>1949 : Hebb&rsquo;s rule</li>
<li>1958 : The perceptron (F. Rosenblatt)</li>
<li>1969 : Limits of perceptrons (M. Minsky)</li>
</ul>

</section>
				<section>

<h2 id="a-biological-inspiration">A biological inspiration</h2>

<p><img src="images/neuron.png" alt="Neuron" /></p>

</section>
				<section>

<h2 id="mcculloch-pitts-formal-neuron">McCulloch &amp; Pitts&rsquo; formal neuron</h2>

<p><img src="images/neuron_model.jpeg" alt="Formal neuron model" /></p>

</section>
				<section>

<h2 id="hebb-s-rule">Hebb&rsquo;s rule</h2>

<p>Attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process.</p>

<blockquote>
<p>&ldquo;The general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become &lsquo;associated&rsquo; so that activity in one facilitates activity in the other.&rdquo;</p>
</blockquote>

</section>
				<section>

<h2 id="franck-rosenblatt-s-perceptron">Franck Rosenblatt&rsquo;s perceptron</h2>

<p><img src="images/Perceptron.jpg" alt="The Perceptron" /></p>

</section>
				<section>

<h2 id="the-perceptron-learning-algorithm">The perceptron learning algorithm</h2>

<ol>
<li>Init randomly the <code>$ \omega $</code> connection weights</li>
<li>For each training sample <code>$ x^{(i)} $</code> :

<ol>
<li>Compute the perceptron output <code>$ y'^{(i)} $</code></li>
<li>Adjust weights : <code>$ \omega_{next} = \omega + \eta (y^{(i)} - y'^{(i)}) x^{(i)} $</code></li>
</ol></li>
</ol>

</section>
				<section>

<h2 id="multilayer-perceptron-mlp">MultiLayer Perceptron (MLP)</h2>

<p><img src="images/neural_net2.jpeg" alt="MultiLayer Perceptron" /></p>

</section>
				<section>

<h2 id="minsky-s-critic">Minsky&rsquo;s critic</h2>

<p>One perceptron cannot learn non-linearly separable functions.</p>

<p><img src="images/xor.png" alt="XOR problem" /></p>

<p>At the time, no learning algorithm existed for training the hidden layers of a MLP.</p>

</section>

</section>
				<section>

<h2 id="decisive-breakthroughs">Decisive breakthroughs</h2>

<ul>
<li>1974 : Backpropagation theory (P. Werbos)</li>
<li>1986 : Learning through backpropagation (Rumelhart, Hinton, Williams)</li>
<li>1989 : Universal approximation theorem (Hornik, Stinchcombe, White)</li>
<li>1989 : first researchs on deep neural nets (LeCun, Bengio)</li>
</ul>

</section>
				<section>

<h2 id="the-deep-learning-tsunami">The deep learning tsunami</h2>

<ul>
<li>2012 : AlexNet (Krizhevsky, Sutskever, Hinton) wins the ImageNet competition</li>
<li>2016 : AlphaGo (DeepMind) beats Go master Lee Sedol by 4 victories to 1</li>
<li>2017 : AlphaZero reaches a surhuman level at Go and chess in less than 24h hours</li>
<li>&hellip;</li>
</ul>

</section>
				<section>

<h2 id="anatomy-of-a-network">Anatomy of a network</h2>

<p><img src="images/nn_weights.png" alt="A neural network" /></p>

</section>
				<section>

<section data-shortcode-section>
<h2 id="learning-algorithm">Learning algorithm</h2>

<p><a href="https://www.manning.com/books/deep-learning-with-python"><img src="images/nn_learning.jpg" alt="Extract from the book Deep Learning with Python" /></a></p>

</section>
				<section>

<h2 id="training-and-inference">Training and inference</h2>

<p><img src="images/training_inference1.png" alt="Training and inference" /></p>

</section>

</section>
				<section>

<h2 id="neuron-output">Neuron output</h2>

<p><img src="images/neuron_output.png" alt="Neuron output" /></p>

</section>
				<section>

<h2 id="activation-functions">Activation functions</h2>

<p>They must be non-linear</p>

<p><img src="images/activation_functions.png" alt="Activation functions" /></p>

</section>
				<section>

<h2 id="weights-initialization">Weights initialization</h2>

<p>To facilitate training, they must be:</p>

<ul>
<li>Non-zero</li>
<li>Random</li>
<li>Have small values</li>
</ul>

<p><a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">Several techniques exist</a>.</p>

</section>
				<section>

<section data-shortcode-section>
<h2 id="vectorization-of-computations">Vectorization of computations</h2>

<p><img src="images/nn_matrixes.png" alt="NN matrixes" /></p>

</section>
				<section>

<h2 id="layer-1-output">Layer 1 output</h2>

<p><img src="images/output_layer1.png" alt="Layer 1 output" /></p>

</section>
				<section>

<h2 id="layer-2-output">Layer 2 output</h2>

<p><img src="images/output_layer2.png" alt="Layer 2 output" /></p>

</section>
				<section>

<h2 id="layer-3-output">Layer 3 output</h2>

<p><img src="images/output_layer3.png" alt="Layer 3 output" /></p>

</section>

</section>
				<section>

<h2 id="weights-update">Weights update</h2>

<p>Objective: minimize the loss function</p>

<p>Method : <a href="https://www.bpesquet.fr/en/slides/ai/ml-fundamentals/">gradient descent</a></p>

<p><code>$$\theta_{next} = \theta - \eta\nabla_{\theta}\mathcal{L}(\theta)$$</code></p>

</section>
				<section>

<h2 id="backpropagation">Backpropagation</h2>

<p>Objective: compute the loss function gradient wrt all its parameters (the network weights).</p>

<p>Method: apply the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> to compute partial derivatives backwards, starting from the current output.</p>

<p><a href="https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/">Backprop explained visually</a></p>

</section>
				<section>

<h2 id="demo-time">Demo time!</h2>

<p><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.36248&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;showTestData_hide=true&amp;noise_hide=true&amp;discretize_hide=true&amp;regularization_hide=true&amp;batchSize_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true">A Neural Network Playground (Simplified)</a></p>

</section>
				<section>

<h2 id="neural-network-tuning">Neural Network Tuning</h2>

</section>
				<section>

<h2 id="hyperparameters-choice">Hyperparameters choice</h2>

<ul>
<li>Number of layers</li>
<li>Number of neurons on hidden layers</li>
<li>activation functions</li>
<li>Learning rate</li>
<li>Mini-batch size</li>
<li>&hellip;</li>
</ul>

<p>(Iterative process)</p>

</section>
				<section>

<section data-shortcode-section>
<h2 id="gradient-descent-optimization">Gradient Descent optimization</h2>

<p><a href="https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9"><img src="images/gradient_descent_evolution_map.jpeg" alt="Gradient Descent evolution map" /></a></p>

</section>
				<section>

<h2 id="momentum">Momentum</h2>

<p>Increase the descent speed in the direction of the minimum.</p>

<p>Image: a ball rolling down on a hill.</p>

<p><img src="images/momentum.png" alt="Momentum" /></p>

</section>
				<section>

<h2 id="other-techniques">Other techniques</h2>

<ul>
<li><strong>RMSprop</strong> (<em>Root Mean Square Prop</em>) : use the previous gradients to update the <em>learning rate</em>.</li>
<li><strong>Adam</strong> (<em>Adaptive Moment Estimation</em>) : combines Momentum and RMSprop</li>
</ul>

<p><a href="http://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a></p>

</section>

</section>
				<section>

<section data-shortcode-section>
<h2 id="optimization-generalization">Optimization/generalization</h2>

<ul>
<li><p>Tackle underfitting:</p>

<ul>
<li>Use a more complex network</li>
<li>Train the network longer</li>
</ul></li>

<li><p>Tackle overfitting:</p>

<ul>
<li>Use more training data</li>
<li>Limit the network size</li>
<li>Introduce regularization</li>
<li>Introduce dropout</li>
</ul></li>
</ul>

</section>
				<section>

<h2 id="regularization">Regularization</h2>

<p>Limit weights values by adding a penalty to the loss function.</p>

<ul>
<li><strong>L1</strong> : <code>$ \frac{\lambda}{m} {\sum |{\theta_{ij}}|} $</code></li>
<li><strong>L2</strong> : <code>$ \frac{\lambda}{m} {\sum {\theta_{ij}}^2} $</code></li>
</ul>

<p><code>$ \lambda $</code> is called <strong>regularization rate</strong>.</p>

<pre><code class="language-python"># Add L2 regularization to a layer
model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001),
                activation='relu', input_shape=(10000,)))
</code></pre>

</section>
				<section>

<h2 id="dropout">Dropout</h2>

<p>During training, some weights are randomly set to 0. The network must adapt and become more generic.</p>

<pre><code class="language-python"># Add a layer with 50% dropout during training
model.add(Dense(16, activation='relu', input_shape=(10000,)))
model.add(Dropout(0.5))
</code></pre>

<p><img src="images/dropout.png" alt="Dropout" /></p>

</section>

</section>
				<section>

<h2 id="demo-time-1">Demo time!</h2>

<p><a href="https://playground.tensorflow.org">A Neural Network Playground (Complete)</a></p>
</section>
				</div>
	</div>
	<div>
		<a href="https://ensc.bordeaux-inp.fr">
			<img src="https://www.bpesquet.fr/images/ENSC.jpg" style="position: absolute;
						bottom: 10px;
						left: 10px;" />
		</a>
	</div>

	<script src="https://www.bpesquet.fr/reveal.js/lib/js/head.min.js"></script>
	<script src="https://www.bpesquet.fr/reveal.js/js/reveal.js"></script>

	<script>
		
		
		
		Reveal.initialize({
			
			history: false,
			
			slideNumber: true,
			
			transition: "convex",
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				config: 'TeX-MML-AM_HTMLorMML'  
			},
			dependencies: [
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/markdown\/marked.js' },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/markdown\/markdown.js' },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/notes\/notes.js', async: true },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/zoom-js\/zoom.js', async: true },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/highlight\/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'https:\/\/www.bpesquet.fr\/reveal.js\/plugin\/math\/math.js', async: true }
			]
		});
	</script>
</body>

</html>