<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Introduction au Deep Learning - Baptiste Pesquet</title>

	<link rel="stylesheet" href="../reveal.js/css/reveal.css">
	<link rel="stylesheet" href="../reveal.js/css/theme/white.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Introduction au Deep Learning</h2>
				<p>
					<a href="https://ensc.bordeaux-inp.fr">Ecole Nationale Supérieure de Cognitique</a>
				</p>
				<p>
					<a href="http://bpesquet.com">Baptiste Pesquet</a>
				</p>
			</section>
			<section>
				<img data-src="images/deep_learning_meme_keras.png" />
			</section>
			<section>
				<h3>Sommaire</h3>
				<ul>
					<li>IA, Machine Learning et Deep Learning</li>
					<li>Python et la Data Science</li>
					<li>Les réseaux de neurones</li>
					<li>Premiers pas avec Keras</li>
					<li>Paramétrage et optimisation d'un réseau</li>
				</ul>
			</section>
			<section>
				<h2>IA, Machine Learning et Deep Learning</h2>
			</section>
			<section>
				<a href="http://www.prowesscorp.com/whats-the-difference-between-artificial-intelligence-ai-machine-learning-and-deep-learning/">
					<img data-src="images/ai_ml_dl.png" />
				</a>
			</section>
			<section>
				<section>
					<h3>Intelligence artificielle</h3>
					<blockquote>"L'IA consiste à parvenir à faire faire aux machines ce que l’homme fait aujourd’hui mieux qu’elles, notamment s’adapter,
						apprendre, communiquer et interagir d’une manière riche et variée avec leur environnement."</blockquote>
					(Source :
					<a href="https://www.economie.gouv.fr/France-IA-intelligence-artificielle">#FranceIA</a>)
				</section>
				<section>
					<h3>De nombreux domaines d'étude</h3>
					<ul>
						<li>Logique et raisonnement</li>
						<li>Bases de connaissances, Web sémantique</li>
						<li>Apprentissage automatique</li>
						<li>Traitement du langage naturel</li>
						<li>Robotique</li>
						<li>Sciences cognitives</li>
						<li>Aide à la décision</li>
						<li>...</li>
					</ul>
					<p>(Source :
						<a href="https://www.inria.fr/actualite/actualites-inria/livre-blanc-sur-l-intelligence-artificielle">INRIA</a>)
					</p>
				</section>
				<section>
					<h3>Une histoire jeune et mouvementée</h3>
					<a href="https://www.slideshare.net/dlavenda/ai-and-productivity">
						<img data-src="images/ai_timeline.jpg" />
					</a>
				</section>
				<section>
					<h3>L'été, jusqu'à quand ?</h3>
					<img data-src="images/winteriscoming.jpg" />
				</section>
			</section>
			<section>
				<section>
					<h3>Machine Learning</h3>
					(Apprentissage automatique)
					<p>Ensemble de techniques permettant à des machines de
						<strong>s’entraîner</strong> sur des bases d’exemples, d’en faire émerger des traits, de généraliser sur des exemples non
						encore rencontrés et de
						<strong>s'améliorer</strong> continuellement avec l’expérience.</p>
				</section>
				<section>
					<h3>Un nouveau paradigme</h3>
					<p class="fragment">
						<img data-src="images/programming_paradigm.png" />
					</p>
					<p class="fragment">
						<img data-src="images/training_paradigm.png" />
					</p>
				</section>
				<section>
					<a href="https://xkcd.com/1838/">
						<img data-src="images/machine_learning_xkcd.png" />
					</a>
				</section>
			</section>
			<section>
				<img data-src="images/machine_learning_tree.png" />
			</section>
			<section>
				<section>
					<h3>Apprentissage supervisé</h3>
					<p>Les résultats à obtenir sont fournis avec les données d'entraînement.</p>
					<p>Ces données sont dites
						<strong>structurées</strong> (étiquetées).
					</p>
				</section>
				<section>
					<h3>Régression</h3>
					<p>Le système prédit des valeurs
						<strong>continues</strong>.
					</p>
					<img data-src="images/ml_regression.png"></img>
				</section>
				<section>
					<h3>Classification</h3>
					<p>Le système prédit des valeurs discrètes : il
						<strong>catégorise</strong> les entrées.
					</p>
					<img data-src="images/ml_classification.png" />
				</section>
			</section>
			<section>
				<h3>Apprentissage non supervisé</h3>
				<p>Le système doit découvrir par lui-même une éventuelle structure dans les données fournies.</p>
				<img data-src="images/ml_unsupervised.png" />
			</section>
			<section>
				<h3>Apprentissage par renforcement</h3>
				<p>Les décisions du système lui procurent une
					<strong>récompense</strong> qu'il cherche à maximiser.</p>
				<iframe width="560" height="315" src="https://www.youtube.com/embed/TmPfTpjtdgg?rel=0&amp;showinfo=0" frameborder="0" gesture="media"
				 allow="encrypted-media" allowfullscreen></iframe>
			</section>
			<section>
				<section>
					<h3>Deep Learning</h3>
					(Appprentissage profond)
					<p>Ensemble de techniques d’apprentissage automatique dans lesquels de vastes réseaux de neurones artificiels exploitent
						de grandes quantités de données.</p>
					<p>A l'origine de la majorité des avancées récentes en IA.</p>
				</section>
				<section>
					<h3>Reconnaissance visuelle</h3>
					<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">
						<img data-src="images/computer_vision.png"></img>
					</a>
				</section>
				<section>
					<h3>Compréhension de la parole</h3>
					<img data-src="images/apple_siri.jpg" />
				</section>
				<section>
					<h3>Véhicules autonomes</h3>
					<iframe data-src="https://player.vimeo.com/video/192261894?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0"
					 webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
				</section>
				<section>
					<h3>Algorithmes de recommandation</h3>
					<img data-src="images/discover_weekly.png" />
				</section>
				<section>
					<h3>Jeux complexes</h3>
					<img data-src="images/alphago.png" />
				</section>
			</section>
			<section>
				<section>
					<h3>Les clés du succès</h3>
					<ul>
						<li>L'optimisation d'algorithmes connus depuis les années 1980</li>
						<li>L'augmentation de la puissance de calcul des machines</li>
						<li>L'explosion du volume de données disponibles</li>
					</ul>
				</section>
				<section>
					<h3>L'état de l'art au début des années 1990</h3>
					<img data-src="images/lecun_lenet.gif" />
					<p>
						<a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a>
					</p>
				</section>
				<section>
					<img data-src="images/infographic2-intel-past-present.gif" />
				</section>
				<section>
					<img data-src="images/big_data_universe.png" />
				</section>
			</section>
			<section>
				<h2>Python et la Data Science</h2>
			</section>
			<section>
				<h3>Le langage Python</h3>
				<p>Langage de programmation sous licence libre créé en 1989 par Guido van Rossum.</p>
				<img data-src="images/Python_logo.png" />
				<ul>
					<li>Multi-paradigmes (procédural, fonctionnel, objet)</li>
					<li>Typage dynamique</li>
					<li>Gestion automatique de la mémoire</li>
				</ul>
			</section>
			<section>
				<h3>Python, le standard pour la Data Science</h3>
				<ul>
					<li>Qualités du langage (logique, simplicité, souplesse)</li>
					<li>Implication de la communauté scientifique et universitaire</li>
					<li>Existence de nombreuses bibliothèques dédiées</li>
				</ul>
			</section>
			<section>
				<h3>En pratique</h3>
				<a href="https://github.com/bpesquet/intro-deep-learning/blob/master/1%20-%20Python%20For%20Data%20Science.ipynb">Python For Data Science</a>
			</section>
			<section>
				<h2>Les réseaux de neurones</h2>
			</section>
			<section>
				<section>
					<h3>Les origines</h3>
					<ul>
						<li>1943 : premier modèle mathématique du neurone biologique (McCulloch et Pitts)</li>
						<li>1949 : règle de Hebb</li>
						<li>1958 : le perceptron (F. Rosenblatt)</li>
						<li>1969 : limites des perceptrons (M. Minsky)</li>
					</ul>
				</section>
				<section>
					<h3>L'inspiration biologique</h3>
					<img data-src="images/neuron.png" />
				</section>
				<section>
					<h3>Le neurone formel de McCulloch et Pitts</h3>
					<a href="http://cs231n.github.io/neural-networks-1/">
						<img data-src="images/neuron_model.jpeg" />
					</a>
				</section>
				<section>
					<h3>La règle de Hebb</h3>
					<p>Postulat sur l'importance des synapses entre neurones pour l'apprentissage.</p>
					<q>
						Quand un axone d'une cellule A est assez proche pour exciter une cellule B de manière répétée et persistante, une croissance
						ou des changements métaboliques prennent place dans l'une ou les deux cellules ce qui entraîne une augmentation de
						l'efficacité de A comme cellule stimulant B.
					</q>
				</section>
				<section>
					<h3>Le perceptron de Franck Rosenblatt</h3>
					<img data-src="images/Perceptron.jpg" />
				</section>
				<section>
					<h3>Algorithme d'apprentissage du perceptron</h3>
					<ol>
						<li>Initialiser aléatoirement les poids des connexions</li>
						<li>Pour chaque donnée de test :
							<ol>
								<li>Calculer la sortie du perceptron</li>
								<li>Si la sortie calculée diffère de celle attendue :
									<ul>
										<li>Si 0 au lieu de 1, diminuer les poids ayant 1 comme valeur d'entrée</li>
										<li>Si 1 au lieu de 0, augmenter les poids ayant 1 comme valeur d'entrée</li>
									</ul>
								</li>
							</ol>
						</li>
					</ol>
				</section>
				<section>
					<h3>Multi Layer Perceptron (MLP)</h3>
					<a href="http://cs231n.github.io/neural-networks-1/">
						<img data-src="images/neural_net2.jpeg" />
					</a>
				</section>
				<section>
					<h3>La critique de Minsky</h3>
					<ul>
						<li>Un seul perceptron ne peut pas apprendre une fonction non séparable linéairement</li>
					</ul>
					<img data-src="images/xor.png" />
					<ul>
						<li>Aucun algorithme d'apprentissage ne fonctionne pour les couches cachées d'un MLP</li>
					</ul>

				</section>
			</section>
			<section>
				<section>
					<h3>Des progrès décisifs</h3>
					<ul>
						<li>1974 : technique de la rétropropagation (P. Werbos)</li>
						<li>1986 : apprentissage par rétropropagation (Rumelhart, Hinton, Williams)</li>
						<li>1989 : preuve mathématique de la capacité des réseaux de neurones à approximer toute fonction mesurable (Hornik, Stinchcombe,
							White)
						</li>
						<li>1989 : début des travaux sur les réseaux convolutifs profonds (Y. LeCun, Y. Bengio)</li>
					</ul>
				</section>
				<section>
					<h3>Les prémisses de la reconnaissance visuelle</h3>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/FwFduRA_L6Q?showinfo=0" frameborder="0" gesture="media"
					 allow="encrypted-media" allowfullscreen></iframe>
				</section>
			</section>
			<section>
				<h3>L'avènement du Deep Learning</h3>
				<ul>
					<li>2012 : AlexNet (Krizhevsky, Sutskever, Hinton) remporte la compétition ImageNet</li>
					<li>2016 : AlphaGo (DeepMind) bat le champion de Go Lee Sedol par 4 victoires à 1</li>
					<li>2017 : AlphaZero atteint en 24h un niveau surhumain au Go et aux échecs</li>
				</ul>
			</section>
			<section>
				<h3>Apprentissage supervisé</h3>
				<ul>
					<li class="fragment">
						<strong>Classification binaire</strong> (0 ou 1)
						<br>Chat/non chat, spam/non spam, maligne/bénigne
					</li>
					<li class="fragment">
						<strong>Classification multiclasses</strong>
						<br>Chat/chien/autre animal, reconnaissance de chiffres, catégorisation de tweets
					</li>
					<li class="fragment">
						<strong>Régression</strong>
						<br>Prix d'un bien immobilier, prévision de températures, âge d'une personne
					</li>
				</ul>
			</section>
			<section>
				<h3>Anatomie d'un réseau</h3>
				<img data-src="images/nn_weights.png" />
			</section>
			<section>
				<h3>Algorithme d'apprentissage</h3>
				<a href="https://www.manning.com/books/deep-learning-with-python">
					<img data-src="images/01fig09.jpg" />
				</a>
			</section>
			<section>
				<section>
					<h3>Format des données d'entrée</h3>
					<ul>
						<li>
							<strong>Cas général</strong> : matrice de la forme
							<em>(samples, features)</em>
						</li>
						<li>
							<strong>Images</strong> : tenseur 4D de la forme
							<em>(samples, height, width, color_channels)</em>
						</li>
						<li>
							<strong>Vidéos</strong> : tenseur 5D de la forme
							<em>(samples, frames, height, width, color_channels)</em>
						</li>
					</ul>
				</section>
				<section>
					<h3>Exemple : prix immobiliers</h3>
					<table>
						<thead>
							<th>Surface (m2)
							</th>
							<th>Nombre chambres
							</th>
							<th>Age (années)
							</th>
							<th>Code Postal
							</th>
							<th>Prix (k€)
							</th>
						</thead>
						<tbody>
							<tr>
								<td>145</td>
								<td>6</td>
								<td>32</td>
								<td>33600</td>
								<td>380</td>
							</tr>
							<tr>
								<td>85</td>
								<td>3</td>
								<td>45</td>
								<td>33700</td>
								<td>290</td>
							</tr>
							<tr>
								<td>210</td>
								<td>7</td>
								<td>12</td>
								<td>33400</td>
								<td>740</td>
							</tr>
							<tr>
								<td>...</td>
								<td>...</td>
								<td>...</td>
								<td>...</td>
								<td>...</td>
							</tr>
							<tr>
								<td>110</td>
								<td>4</td>
								<td>22</td>
								<td>33000</td>
								<td>410</td>
							</tr>
						</tbody>
					</table>
				</section>
				<section>
					<h3>Exemple : prix immobiliers</h3>
					<img data-src="images/nn_data_matrix.png" />
				</section>
				<section>
					<h3>Exemple : image RGB</h3>
					<img data-src="images/nn_image_channels.png" />
				</section>
			</section>
			<section>
				<h3>Sortie pour un neurone</h3>
				<img data-src="images/neuron_output.png" />
			</section>
			<section>
				<h3>Les fonctions d'activation</h3>
				<img data-src="images/activation_functions.png" />
			</section>
			<section>
				<section>
					<h3>Vectorisation des calculs</h3>
					<img data-src="images/nn_matrixes.png" />
				</section>
				<section>
					<h3>Sortie de la couche 1</h3>
					<img data-src="images/output_layer1.png" />
				</section>
				<section>
					<h3>Sortie de la couche 2</h3>
					<img data-src="images/output_layer2.png" />
				</section>
				<section>
					<h3>Sortie de la couche 3</h3>
					<img data-src="images/output_layer3.png" />
				</section>
			</section>
			<section>
				<section>
					<h3>La fonction de coût (loss)</h3>
					<ul>
						<li>Mesure l'écart entre les résultats attendus et calculés</li>
						<li>Dépend du type d'apprentissage</li>
					</ul>
				</section>
				<section>
					<h3>Exemples de fonctions</h3>
					<ul>
						<li>
							<em>Binary crossentropy</em>
							<br>\(\mathcal{L}(\theta)= -\frac{1}{m}\sum_{i=1}^m \left[y_i \log(y'_i) + (1-y_i) \log(1-y'_i)\right]\)
						</li>
						<li>
							<em>Categorical crossentropy</em>
							<br>\(\mathcal{L}(\theta)= -\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^n y_{ij} \log(y'_{ij})\)
						</li>
						<li>
							<em>Mean Squared Error</em>
							<br>\(\mathcal{L}(\theta)= \frac{1}{m}\sum_{i=1}^m (y'_{i} - y_{i})^2\)
						</li>
					</ul>
				</section>
			</section>
			<section>
				<h3>Exemple en Python</h3>
				<pre><code data-trim>
class NeuralNetwork(object):
"""Neural network with one hidden layer"""

def __init__(self, input_size, hidden_size):
  # store the sizes of the 3 layers
  self.input_size = input_size
  self.hidden_size = hidden_size
  self.output_size = 1

  # weight matrix from input to hidden layer
  self.W1 = np.random.randn(
    self.input_size, self.hidden_size) / 
      np.sqrt(self.input_size)
  self.b1 = np.zeros((1, self.hidden_size))
  # weight matrix from hidden to output layer
  self.W2 = np.random.randn(
    self.hidden_size, self.output_size) / 
	  np.sqrt(self.hidden_size)
  self.b2 = np.zeros((1, self.output_size))
				</code></pre>
			</section>
		</div>
	</div>
	<div>
		<a href="https://ensc.bordeaux-inp.fr">
			<img src="images/ENSC.jpg" style="position: absolute;
								bottom: 10px;
								left: 10px;" />
		</a>
	</div>

	<script src="../reveal.js/lib/js/head.min.js"></script>
	<script src="../reveal.js/js/reveal.js"></script>

	<script>
		// More info about config & dependencies:
		// - https://github.com/hakimel/reveal.js#configuration
		// - https://github.com/hakimel/reveal.js#dependencies
		Reveal.initialize({
			// Display the page number of the current slide
			slideNumber: true,
			// Display the page number of the current slide
			transition: "convex",
			// Parallax background image
			parallaxBackgroundImage: 'images/draft.png',
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				config: 'TeX-MML-AM_HTMLorMML'  // See http://docs.mathjax.org/en/latest/config-files.html
			},
			dependencies: [
				{ src: '../reveal.js/plugin/markdown/marked.js' },
				{ src: '../reveal.js/plugin/markdown/markdown.js' },
				{ src: '../reveal.js/plugin/notes/notes.js', async: true },
				{ src: '../reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: '../reveal.js/plugin/math/math.js', async: true }
			]
		});
	</script>
</body>

</html>