<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Introduction au Deep Learning - Baptiste Pesquet</title>

	<link rel="stylesheet" href="../reveal.js/css/reveal.css">
	<link rel="stylesheet" href="../reveal.js/css/theme/white.css">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<h2>Introduction au Deep Learning</h2>
				<p>
					<a href="https://ensc.bordeaux-inp.fr">Ecole Nationale Supérieure de Cognitique</a>
				</p>
				<p>
					<a href="http://bpesquet.com">Baptiste Pesquet</a>
				</p>
			</section>
			<section>
				<img data-src="images/deep_learning_meme_keras.png" />
			</section>
			<section>
				<h3>Sommaire</h3>
				<ul>
					<li>IA, Machine Learning et Deep Learning</li>
					<li>Python et la Data Science</li>
					<li>Les réseaux de neurones</li>
					<li>Premiers pas avec Keras</li>
					<li>Paramétrage et optimisation d'un réseau</li>
				</ul>
			</section>
			<section>
				<h2>IA, Machine Learning et Deep Learning</h2>
			</section>
			<section>
				<a href="http://www.prowesscorp.com/whats-the-difference-between-artificial-intelligence-ai-machine-learning-and-deep-learning/">
					<img data-src="images/ai_ml_dl.png" />
				</a>
			</section>
			<section>
				<section>
					<h3>Intelligence artificielle</h3>
					<blockquote>"L'IA consiste à parvenir à faire faire aux machines ce que l’homme fait aujourd’hui mieux qu’elles, notamment s’adapter,
						apprendre, communiquer et interagir d’une manière riche et variée avec leur environnement."</blockquote>
					(Source :
					<a href="https://www.economie.gouv.fr/France-IA-intelligence-artificielle">#FranceIA</a>)
				</section>
				<section>
					<h3>De nombreux domaines d'étude</h3>
					<ul>
						<li>Logique et raisonnement</li>
						<li>Bases de connaissances, Web sémantique</li>
						<li>Apprentissage automatique</li>
						<li>Traitement du langage naturel</li>
						<li>Robotique</li>
						<li>Sciences cognitives</li>
						<li>Aide à la décision</li>
						<li>...</li>
					</ul>
					<p>(Source :
						<a href="https://www.inria.fr/actualite/actualites-inria/livre-blanc-sur-l-intelligence-artificielle">INRIA</a>)
					</p>
				</section>
				<section>
					<h3>Une histoire jeune et mouvementée</h3>
					<a href="https://www.slideshare.net/dlavenda/ai-and-productivity">
						<img data-src="images/ai_timeline.jpg" />
					</a>
				</section>
				<section>
					<h3>L'été, jusqu'à quand ?</h3>
					<img data-src="images/winteriscoming.jpg" />
				</section>
			</section>
			<section>
				<section>
					<h3>Machine Learning</h3>
					(Apprentissage automatique)
					<p>Ensemble de techniques permettant à des machines de
						<strong>s’entraîner</strong> sur des bases d’exemples, d’en faire émerger des traits, de généraliser sur des exemples non
						encore rencontrés et de
						<strong>s'améliorer</strong> continuellement avec l’expérience.</p>
				</section>
				<section>
					<h3>Un nouveau paradigme</h3>
					<p class="fragment">
						<img data-src="images/programming_paradigm.png" />
					</p>
					<p class="fragment">
						<img data-src="images/training_paradigm.png" />
					</p>
				</section>
				<section>
					<a href="https://xkcd.com/1838/">
						<img data-src="images/machine_learning_xkcd.png" />
					</a>
				</section>
			</section>
			<section>
				<img data-src="images/machine_learning_tree.png" />
			</section>
			<section>
				<section>
					<h3>Apprentissage supervisé</h3>
					<p>Les résultats à obtenir sont fournis avec les données d'entraînement.</p>
					<p>Ces données sont dites
						<strong>structurées</strong> (étiquetées).
					</p>
				</section>
				<section>
					<h3>Régression</h3>
					<p>Le système prédit des valeurs
						<strong>continues</strong>.
					</p>
					<img data-src="images/ml_regression.png"></img>
				</section>
				<section>
					<h3>Classification</h3>
					<p>Le système prédit des valeurs discrètes : il
						<strong>catégorise</strong> les entrées.
					</p>
					<img data-src="images/ml_classification.png" />
				</section>
			</section>
			<section>
				<h3>Apprentissage non supervisé</h3>
				<p>Le système doit découvrir par lui-même une éventuelle structure dans les données fournies.</p>
				<img data-src="images/ml_unsupervised.png" />
			</section>
			<section>
				<h3>Apprentissage par renforcement</h3>
				<p>Les décisions du système lui procurent une
					<strong>récompense</strong> qu'il cherche à maximiser.</p>
				<iframe width="560" height="315" src="https://www.youtube.com/embed/TmPfTpjtdgg?rel=0&amp;showinfo=0" frameborder="0" gesture="media"
				 allow="encrypted-media" allowfullscreen></iframe>
			</section>
			<section>
				<section>
					<h3>Deep Learning</h3>
					(Appprentissage profond)
					<p>Ensemble de techniques d’apprentissage automatique dans lesquels de vastes réseaux de neurones artificiels exploitent
						de grandes quantités de données.</p>
					<p>A l'origine de la majorité des avancées récentes en IA.</p>
				</section>
				<section>
					<h3>Reconnaissance visuelle</h3>
					<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">
						<img data-src="images/computer_vision.png"></img>
					</a>
				</section>
				<section>
					<h3>Compréhension de la parole</h3>
					<img data-src="images/apple_siri.jpg" />
				</section>
				<section>
					<h3>Véhicules autonomes</h3>
					<iframe data-src="https://player.vimeo.com/video/192261894?title=0&byline=0&portrait=0" width="640" height="360" frameborder="0"
					 webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
				</section>
				<section>
					<h3>Algorithmes de recommandation</h3>
					<img data-src="images/discover_weekly.png" />
				</section>
				<section>
					<h3>Jeux complexes</h3>
					<img data-src="images/alphago.png" />
				</section>
			</section>
			<section>
				<section>
					<h3>Les clés du succès</h3>
					<ul>
						<li>L'optimisation d'algorithmes connus depuis les années 1980</li>
						<li>L'augmentation de la puissance de calcul des machines</li>
						<li>L'explosion du volume de données disponibles</li>
					</ul>
				</section>
				<section>
					<h3>L'état de l'art au début des années 1990</h3>
					<img data-src="images/lecun_lenet.gif" />
					<p>
						<a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a>
					</p>
				</section>
				<section>
					<img data-src="images/infographic2-intel-past-present.gif" />
				</section>
				<section>
					<img data-src="images/big_data_universe.png" />
				</section>
			</section>
			<section>
				<h2>Python et la Data Science</h2>
			</section>
			<section>
				<h3>Le langage Python</h3>
				<p>Langage de programmation sous licence libre créé en 1989 par Guido van Rossum.</p>
				<img data-src="images/Python_logo.png" />
				<ul>
					<li>Multi-paradigmes (procédural, fonctionnel, objet)</li>
					<li>Typage dynamique</li>
					<li>Gestion automatique de la mémoire</li>
				</ul>
			</section>
			<section>
				<h3>Python, le standard pour la Data Science</h3>
				<ul>
					<li>Qualités du langage (logique, simplicité, souplesse)</li>
					<li>Implication de la communauté scientifique et universitaire</li>
					<li>Existence de nombreuses bibliothèques dédiées</li>
				</ul>
			</section>
			<section>
				<h3>En pratique</h3>
				<a href="https://github.com/bpesquet/intro-deep-learning/blob/master/1%20-%20Python%20For%20Data%20Science.ipynb">Python For Data Science</a>
			</section>
			<section>
				<h2>Les réseaux de neurones</h2>
			</section>
			<section>
				<section>
					<h3>Les origines</h3>
					<ul>
						<li>1943 : premier modèle mathématique du neurone biologique (McCulloch et Pitts)</li>
						<li>1949 : règle de Hebb</li>
						<li>1958 : le perceptron (F. Rosenblatt)</li>
						<li>1969 : limites des perceptrons (M. Minsky)</li>
					</ul>
				</section>
				<section>
					<h3>L'inspiration biologique</h3>
					<img data-src="images/neuron.png" />
				</section>
				<section>
					<h3>Le neurone formel de McCulloch et Pitts</h3>
					<a href="http://cs231n.github.io/neural-networks-1/">
						<img data-src="images/neuron_model.jpeg" />
					</a>
				</section>
				<section>
					<h3>La règle de Hebb</h3>
					<p>Postulat sur l'importance des synapses entre neurones pour l'apprentissage.</p>
					<q>
						Quand un axone d'une cellule A est assez proche pour exciter une cellule B de manière répétée et persistante, une croissance
						ou des changements métaboliques prennent place dans l'une ou les deux cellules ce qui entraîne une augmentation de
						l'efficacité de A comme cellule stimulant B.
					</q>
				</section>
				<section>
					<h3>Le perceptron de Franck Rosenblatt</h3>
					<img data-src="images/Perceptron.jpg" />
				</section>
				<section>
					<h3>Algorithme d'apprentissage du perceptron</h3>
					<ol>
						<li>Initialiser aléatoirement les poids des connexions</li>
						<li>Pour chaque donnée de test :
							<ol>
								<li>Calculer la sortie du perceptron</li>
								<li>Si la sortie calculée diffère de celle attendue :
									<ul>
										<li>Si 0 au lieu de 1, diminuer les poids ayant 1 comme valeur d'entrée</li>
										<li>Si 1 au lieu de 0, augmenter les poids ayant 1 comme valeur d'entrée</li>
									</ul>
								</li>
							</ol>
						</li>
					</ol>
				</section>
				<section>
					<h3>Multi Layer Perceptron (MLP)</h3>
					<a href="http://cs231n.github.io/neural-networks-1/">
						<img data-src="images/neural_net2.jpeg" />
					</a>
				</section>
				<section>
					<h3>La critique de Minsky</h3>
					<ul>
						<li>Un seul perceptron ne peut pas apprendre une fonction non séparable linéairement</li>
					</ul>
					<img data-src="images/xor.png" />
					<ul>
						<li>Aucun algorithme d'apprentissage ne fonctionne pour les couches cachées d'un MLP</li>
					</ul>

				</section>
			</section>
			<section>
				<section>
					<h3>Des progrès décisifs</h3>
					<ul>
						<li>1974 : technique de la rétropropagation (P. Werbos)</li>
						<li>1986 : apprentissage par rétropropagation (Rumelhart, Hinton, Williams)</li>
						<li>1989 : preuve mathématique de la capacité des réseaux de neurones à approximer toute fonction mesurable (Hornik, Stinchcombe,
							White)
						</li>
						<li>1989 : début des travaux sur les réseaux convolutifs profonds (Y. LeCun, Y. Bengio)</li>
					</ul>
				</section>
				<section>
					<h3>Les prémisses de la reconnaissance visuelle</h3>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/FwFduRA_L6Q?showinfo=0" frameborder="0" gesture="media"
					 allow="encrypted-media" allowfullscreen></iframe>
				</section>
			</section>
			<section>
				<h3>L'avènement du Deep Learning</h3>
				<ul>
					<li>2012 : AlexNet (Krizhevsky, Sutskever, Hinton) remporte la compétition ImageNet</li>
					<li>2016 : AlphaGo (DeepMind) bat le champion de Go Lee Sedol par 4 victoires à 1</li>
					<li>2017 : AlphaZero atteint en 24h un niveau surhumain au Go et aux échecs</li>
				</ul>
			</section>
			<section>
				<h3>Apprentissage supervisé</h3>
				<ul>
					<li class="fragment">
						<strong>Classification binaire</strong> (0 ou 1)
						<br>Chat/non chat, spam/non spam, maligne/bénigne
					</li>
					<li class="fragment">
						<strong>Classification multiclasses</strong>
						<br>Chat/chien/autre animal, reconnaissance de chiffres, catégorisation de tweets
					</li>
					<li class="fragment">
						<strong>Régression</strong>
						<br>Prix d'un bien immobilier, prévision de températures, âge d'une personne
					</li>
				</ul>
			</section>
			<section>
				<h3>Anatomie d'un réseau</h3>
				<img data-src="images/nn_weights.png" />
			</section>
			<section>
				<section>
					<h3>Algorithme d'apprentissage</h3>
					<a href="https://www.manning.com/books/deep-learning-with-python">
						<img data-src="images/01fig09.jpg" />
					</a>
				</section>
				<section>
					<h3>Entrainement et inférence</h3>
					<img data-src="images/training_inference1.png" />
				</section>
			</section>
			<section>
				<section>
					<h3>Format des données d'entrée</h3>
					<ul>
						<li>
							<strong>Cas général</strong> : matrice de la forme
							<em>(samples, features)</em>
						</li>
						<li>
							<strong>Images</strong> : tenseur 4D de la forme
							<em>(samples, height, width, color_channels)</em>
						</li>
						<li>
							<strong>Vidéos</strong> : tenseur 5D de la forme
							<em>(samples, frames, height, width, color_channels)</em>
						</li>
					</ul>
				</section>
				<section>
					<h3>Exemple : prix immobiliers</h3>
					<table>
						<thead>
							<th>Surface (m2)
							</th>
							<th>Nombre chambres
							</th>
							<th>Age (années)
							</th>
							<th>Code Postal
							</th>
							<th>Prix (k€)
							</th>
						</thead>
						<tbody>
							<tr>
								<td>145</td>
								<td>6</td>
								<td>32</td>
								<td>33600</td>
								<td>380</td>
							</tr>
							<tr>
								<td>85</td>
								<td>3</td>
								<td>45</td>
								<td>33700</td>
								<td>290</td>
							</tr>
							<tr>
								<td>210</td>
								<td>7</td>
								<td>12</td>
								<td>33400</td>
								<td>740</td>
							</tr>
							<tr>
								<td>...</td>
								<td>...</td>
								<td>...</td>
								<td>...</td>
								<td>...</td>
							</tr>
							<tr>
								<td>110</td>
								<td>4</td>
								<td>22</td>
								<td>33000</td>
								<td>410</td>
							</tr>
						</tbody>
					</table>
				</section>
				<section>
					<h3>Exemple : prix immobiliers</h3>
					<img data-src="images/nn_data_matrix.png" />
				</section>
				<section>
					<h3>Exemple : image RGB</h3>
					<img data-src="images/nn_image_channels.png" />
				</section>
			</section>
			<section>
				<h3>Sortie pour un neurone</h3>
				<img data-src="images/neuron_output.png" />
			</section>
			<section>
				<h3>Les fonctions d'activation</h3>
				<img data-src="images/activation_functions.png" />
			</section>
			<section>
				<section>
					<h3>Vectorisation des calculs</h3>
					<img data-src="images/nn_matrixes.png" />
				</section>
				<section>
					<h3>Sortie de la couche 1</h3>
					<img data-src="images/output_layer1.png" />
				</section>
				<section>
					<h3>Sortie de la couche 2</h3>
					<img data-src="images/output_layer2.png" />
				</section>
				<section>
					<h3>Sortie de la couche 3</h3>
					<img data-src="images/output_layer3.png" />
				</section>
			</section>
			<section>
				<section>
					<h3>La fonction de coût (loss)</h3>
					<ul>
						<li>Mesure l'écart entre les résultats attendus et calculés</li>
						<li>Paramètres : les poids du réseau</li>
						<li>Dépend du type d'apprentissage</li>
					</ul>
				</section>
				<section>
					<h3>Exemples de fonctions</h3>
					<ul>
						<li>
							<em>Binary crossentropy</em>
							<br>\(\mathcal{J}(\theta)= -\frac{1}{m}\sum_{i=1}^m \left[y_i \log(y'_i) + (1-y_i) \log(1-y'_i)\right]\)
						</li>
						<li>
							<em>Categorical crossentropy</em>
							<br>\(\mathcal{J}(\theta)= -\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^n y_{ij} \log(y'_{ij})\)
						</li>
						<li>
							<em>Mean Squared Error</em>
							<br>\(\mathcal{J}(\theta)= \frac{1}{m}\sum_{i=1}^m (y'_{i} - y_{i})^2\)
						</li>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h3>Ajustement des poids du réseau</h3>
					<p>Objectif : minimiser la fonction de coût</p>
					<p>Principe : mettre à jour les poids par petites étapes dans la direction inverse du
						<strong>gradient</strong> de la fonction de coût
					</p>
				</section>
				<section>
					<h3>La notion de gradient</h3>
					<p>Variation d'une fonction par rapport à la variation de ses différents paramètres</p>
					<p>Vecteur dont les composantes sont les dérivées partielles de la fonction</p>
				</section>
				<section>
					<h3>Descente de gradient</h3>
					<img data-src="images/gradient_descent_1D.png" />
				</section>
				<section>
					<h3>Gradient 2D</h3>
					<img data-src="images/tangent_space.png" />
				</section>
				<section>
					<h3>Descente de gradient 2D</h3>
					<img data-src="images/gradient_descent.gif" />
				</section>
			</section>
			<section>
				<h3>La rétropropagation</h3>
				<p>Objectif : calculer le gradient de la fonction de coût par rapport à ses paramètres (les poids du réseau)</p>
				<p>Principe : appliquer la règle des dérivations en chaîne ou
					<strong>
						<em>chain rule</em>
					</strong> pour calculer les dérivées partielles de la fonction de coût
				</p>
				$$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$$
			</section>
			<section>
				<section>
					<h3>Optimisation de la descente de gradient</h3>
					<p>
						<strong>Learning rate</strong> \(\alpha\) : facteur d'ajustement des poids en fonction du gradient du coût</p>
					$$\theta_{i+1} = \theta_i - \alpha \frac{\partial}{\partial \theta} \mathcal{J}(\theta_i)$$
				</section>
				<section>
					<h3>Importance du learning rate</h3>
					<img data-src="images/learning_rate.png" />
				</section>
				<section>
					<h3>Le problème des minima locaux</h3>
					<img data-src="images/02fig13.jpg" />
				</section>
				<section>
					<h3>Quelques méthodes d'optimisation</h3>
					<ul>
						<li>Stochastic Gradient Descent</li>
						<li>Mini-batch Gradient Descent (entre 50 et 256 samples)</li>
						<li>Momentum</li>
						<li>RMSProp</li>
						<li>Adam</li>
					</ul>
				</section>
			</section>
			<section>
				<h3>Mesure de performance</h3>
				<p>
					<strong>Accuracy</strong>: % de réussite des prévisions sur un jeu de données
				</p>
			</section>
			<section>
				<h3>En pratique</h3>
				<a href="https://github.com/bpesquet/intro-deep-learning/blob/master/2%20-%20Shallow%20Neural%20Networks%20With%20Python.ipynb">Shallow Neural Networks With Python</a>
			</section>
			<section>
				<h2>Premiers pas avec Keras</h2>
			</section>
			<section>
				<h3>Présentation de Keras</h3>
				<p>Librairie Python de création de réseaux de neurones conçue par François Chollet</p>
				<img data-src="images/keras_logo.png" />
				<p>Fournit une API de haut niveau au-dessus de TensorFlow, Theano ou CNTK</p>
			</section>
			<section>
				<section>
					<h3>Un premier exemple</h3>
					<pre><code data-trim class="python">
from keras.models import Sequential
from keras.layers import Dense

model = Sequential()
model.add(Dense(4, activation='tanh'), input_shape=(3,))
model.add(Dense(4, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))

network.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy'])
history = model.fit(train_X, train_y, epochs=5, batch_size=64)

loss, acc = model.evaluate(test_X, test_y, verbose=0)
					</code></pre>
				</section>
				<section>
					<h3>Définition du réseau</h3>
					<pre><code data-trim class="python">
# Sequential defines a linear stack of layers
from keras.models import Sequential
# Dense defines a fully connected layer
from keras.layers import Dense

model = Sequential() # Create a new network
# Add a 4-neurons layer using tanh as activation function
# Input shape corresponds to the input layer (no of features)
model.add(Dense(4, activation='tanh'), input_shape=(3,))
# Add a 4-neurons layer using tanh
# Input shape is infered from previous layer
model.add(Dense(4, activation='tanh'))
# Add a 1-neuron output layer using sigmoid
model.add(Dense(1, activation='sigmoid'))
					</code></pre>
				</section>
				<section>
					<h3>Réseau obtenu</h3>
					<img data-src="images/neural_net2.jpeg" />
				</section>
				<section>
					<h3>Compilation du réseau</h3>
					<pre><code data-trim class="python">
# Configuration of the training process
# optimizer: gradient descent optimization method
# loss: loss function
# metrics: list of metrics monitored during training and testing
network.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy'])
					</code></pre>
				</section>
				<section>
					<h3>Apprentissage</h3>
					<pre><code data-trim class="python">
# Launch the training of the network on the data
# epochs: number of epochs to train the model
#  (An epoch is an iteration over the entire training dataset)
# batch_size: number of samples used at each training iteration
# The returned history object contains the monitored metrics
history = model.fit(train_X, train_y, epochs=5, batch_size=64)
					</code></pre>
				</section>
				<section>
					<h3>Evaluation</h3>
					<pre><code data-trim class="python">
# Returns the loss value & metrics values for the network in test mode
loss, acc = model.evaluate(test_X, test_y, verbose=0)
					</code></pre>
				</section>
			</section>
			<section>
				<h3>En pratique</h3>
				<a href="https://github.com/bpesquet/intro-deep-learning/blob/master/3%20-%20First%20Steps%20With%20Keras.ipynb">First Steps With Keras</a>
			</section>
		</div>
	</div>
	<div>
		<a href="https://ensc.bordeaux-inp.fr">
			<img src="images/ENSC.jpg" style="position: absolute;
								bottom: 10px;
								left: 10px;" />
		</a>
	</div>

	<script src="../reveal.js/lib/js/head.min.js"></script>
	<script src="../reveal.js/js/reveal.js"></script>

	<script>
		// More info about config & dependencies:
		// - https://github.com/hakimel/reveal.js#configuration
		// - https://github.com/hakimel/reveal.js#dependencies
		Reveal.initialize({
			// Display the page number of the current slide
			slideNumber: true,
			// Display the page number of the current slide
			transition: "convex",
			// Parallax background image
			parallaxBackgroundImage: 'images/draft.png',
			math: {
				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				config: 'TeX-MML-AM_HTMLorMML'  // See http://docs.mathjax.org/en/latest/config-files.html
			},
			dependencies: [
				{ src: '../reveal.js/plugin/markdown/marked.js' },
				{ src: '../reveal.js/plugin/markdown/markdown.js' },
				{ src: '../reveal.js/plugin/notes/notes.js', async: true },
				{ src: '../reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: '../reveal.js/plugin/math/math.js', async: true }
			]
		});
	</script>
</body>

</html>